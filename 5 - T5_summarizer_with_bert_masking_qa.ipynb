{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summarize data with T5 summarization and BERT-Large-Masking-SQUAD question answering\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import sentencepiece\n",
    "import torch\n",
    "\n",
    "from transformers import pipeline, AutoTokenizer, T5Tokenizer, T5ForConditionalGeneration, AutoModelForQuestionAnswering, AutoTokenizer, pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# config variables\n",
    "\n",
    "summary_file = \"reliefweb_summaries.csv\"\n",
    "summarization_model = \"google/flan-t5-base\"\n",
    "qa_model = \"deepset/bert-large-uncased-whole-word-masking-squad2\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the excel data into a pandas dataframe\n",
    "directory_path = os.getcwd()\n",
    "summary_file = os.path.join(directory_path, summary_file)\n",
    "print(summary_file)\n",
    "df = pd.read_csv(summary_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the summarization pipeline and loop through the dataframe to summarize the data \n",
    "# in the source_original_text column and add the summary to the dataframe in a column named source_summary\n",
    "\n",
    "summarizer = T5ForConditionalGeneration.from_pretrained(summarization_model)\n",
    "tokenizer = T5Tokenizer.from_pretrained(summarization_model)\n",
    "\n",
    "scaling_factor = 0.5 #how much we want to scale the summary length\n",
    "\n",
    "df_results = pd.DataFrame(columns=[\"source_text\", \"summary_text\", \"source_text_tokens\", \"summary_text_tokens\", \"max_length\", \"min_length\", \n",
    "                                   \"killed\", \"injured\", \"displaced\", \"affected\", \"location\", \"summary_model used\", \"qa_model used\"])\n",
    "\n",
    "for index, row in df.iterrows():\n",
    "    source_text = row[\"source_original_text\"]\n",
    "    print(\"Original text: \", source_text)\n",
    "\n",
    "    # tokenize the input text\n",
    "    inputs = tokenizer.encode(\"summarize: \" + source_text, return_tensors=\"pt\", max_length=1024, truncation=True)\n",
    "    input_length = len(inputs[0])\n",
    "    print(\"Input Token length: \", input_length)\n",
    "\n",
    "    # scale the summary length based on the input length\n",
    "    max_length = int(input_length * scaling_factor)\n",
    "    min_length = 50\n",
    "    if max_length < 50:\n",
    "        min_length = int(max_length * 0.5)\n",
    "\n",
    "    summary_ids = summarizer.generate(inputs, max_length=max_length, min_length=min_length, length_penalty=2.0, num_beams=4, early_stopping=True)\n",
    "    print(\"Summary Token length: \", len(summary_ids[0]))\n",
    "    summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
    "    print(\"Summary text: \", summary)\n",
    "    df_results.at[index, \"source_text\"] = source_text\n",
    "    df_results.at[index, \"summary_text\"] = summary\n",
    "    df_results.at[index, \"source_text_tokens\"] = input_length\n",
    "    df_results.at[index, \"summary_text_tokens\"] = len(summary_ids[0])\n",
    "    df_results.at[index, \"max_length\"] = max_length\n",
    "    df_results.at[index, \"min_length\"] = min_length\n",
    "    df_results.at[index, \"summary model used\"] = summarization_model\n",
    "\n",
    "# now write an excel file with the summarized data\n",
    "# df_results.to_excel(\"summarized_data_flan_T5_base.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now let's set up a pipeline to answer questions based on the summarized data\n",
    "\n",
    "qa_pipe = pipeline(\"question-answering\", model=qa_model, tokenizer=qa_model)\n",
    "\n",
    "killed_q = \"How many people were killed?\"\n",
    "injured_q= \"How many people were injured?\"\n",
    "missing_q = \"How many people are missing?\"\n",
    "displaced_q = \"How many people were displaced?\"\n",
    "affected_q = \"How many people were affected?\"\n",
    "location_q = \"Where did the event happen?\"\n",
    "df_results.at[index, \"qa model used\"] = qa_model\n",
    "\n",
    "#TODO:  Evaluate the confidence of the answers and only accept answers with a confidence above a certain threshold\n",
    "#TODO:  Consider how to incorporate synonyms into questions (e.g. displaced/homeless/migrant/migration)\n",
    "\n",
    "for index, row in df.iterrows():\n",
    "    source_text = row[\"source_original_text\"]\n",
    "    print(\"Original text: \", source_text)\n",
    "    \n",
    "    killed_a = qa_pipe(question=killed_q, context=source_text)\n",
    "    print(\"Killed: \", killed_a)\n",
    "    injured_a = qa_pipe(question=injured_q, context=source_text)\n",
    "    print(\"Injured: \", injured_a)\n",
    "    missing_a = qa_pipe(question=missing_q, context=source_text)\n",
    "    print(\"Missing: \", missing_a)\n",
    "    displaced_a = qa_pipe(question=displaced_q, context=source_text)\n",
    "    print(\"Displaced: \", displaced_a)\n",
    "    affected_a = qa_pipe(question=affected_q, context=source_text)\n",
    "    print(\"Affected: \", affected_a)\n",
    "    location_a = qa_pipe(question=location_q, context=source_text) \n",
    "    print(\"Location: \", location_a)\n",
    "    \n",
    "    df_results.at[index, \"killed\"] = killed_a\n",
    "    df_results.at[index, \"injured\"] = injured_a\n",
    "    df_results.at[index, \"displaced\"] = displaced_a\n",
    "    df_results.at[index, \"affected\"] = affected_a\n",
    "    df_results.at[index, \"location\"] = location_a\n",
    "\n",
    "\n",
    "    df_results.to_csv(\"5 - T5_summary_bert_masking_qa.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
