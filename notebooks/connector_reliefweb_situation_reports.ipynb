{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0e96a8f1-8453-4aa2-b4ed-74b6dac48210",
   "metadata": {},
   "source": [
    "## New 2/26/2024\n",
    "\n",
    "## This serves as the core logic to extracting content from reliefweb\n",
    "\n",
    "### Once this is finished, it replaces reliefweb_situation_reports\n",
    "\n",
    "## Process Steps\n",
    "1) get the last 500 disaster summaries from reliefweb\n",
    "2) process the json and build a dataframe structured such that:\n",
    "   1) each paragraph has its own row\n",
    "   2) each identified reference url in that paragraph is parsed out along with other metadata\n",
    "3) write resulting dataframe to postgres db - note this completely replaces the previously-existing table\n",
    "\n",
    "## Cautions\n",
    "Occasionally the reliefweb api call will return an error - to the effect that there's some bad chunk or something.\n",
    "I've left that unhandled. Conflicting evidence as to whether it's an intermittent issue, maybe cause by rate limiting,\n",
    " or an issue related to a specific summary's format. Both seem to have been the case at various times.\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a14080ca-4ca1-4280-9d02-c63b37654596",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "import uuid\n",
    "import json\n",
    "import os\n",
    "import uuid\n",
    "import re\n",
    "import importlib\n",
    "\n",
    "from datetime import datetime\n",
    "\n",
    "import sys\n",
    "sys.path.append('utilities')\n",
    "import basic_utilities as utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "70e937c4-4841-473c-a656-d0278be1a287",
   "metadata": {},
   "outputs": [],
   "source": [
    "# key config vars\n",
    "api_endpoint = 'https://api.reliefweb.int/v1/reports?appname=amcross'\n",
    "\n",
    "\n",
    "output_format = 'excel' #other option is 'postgres'\n",
    "output_name = 'rw_disaster_situation_reports'\n",
    "\n",
    "\n",
    "## If you plan to connect to a database\n",
    "db_conf ={\n",
    "    'host':\"xxx\",\n",
    "    'port':'5432',\n",
    "    'database':\"postgres\",\n",
    "    'user':\"postgres\",\n",
    "    'password':\"xxx\"\n",
    "}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5a9a81ba-1169-4abd-9ba9-5e02d2f4b3d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_rw_situation_reports(limit=500):\n",
    "    \n",
    "\n",
    "    #set a high limit for latest in case the job doesn't run for a long time\n",
    "    params = {\n",
    "        'appname': 'amcross','profile': 'full','preset': 'latest','limit': limit\n",
    "        ,'query[fields][]':'format.name','query[value]':'Situation Report'\n",
    "       # ,'filter[field]': \"id\",'filter[value][from]':max_id\n",
    "    }\n",
    "\n",
    "    \n",
    "\n",
    "    # Make the API request\n",
    "    response = requests.get(api_endpoint, params=params)\n",
    "    \n",
    "    # Check the status of the response\n",
    "    if response.status_code == 200:\n",
    "        # Parse and use the response data (in JSON format)\n",
    "        data = response.json()\n",
    "        return data['data']\n",
    "        # for disasters if we don't return everything, can't get to the original api call\n",
    "        #return data\n",
    "    \n",
    "    else:\n",
    "        print(f\"Error: {response.status_code} - {response.text}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "\n",
    "         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "45064492-7d37-46c3-964f-6875d516ecf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_themes(j):\n",
    "    #must pass in after levelling json up to j['fields']\n",
    "    themes = j.get('theme')\n",
    "    if themes is None:\n",
    "        return None\n",
    "        \n",
    "    ts =[]\n",
    "    for theme in themes:\n",
    "        ts.append(theme['name'].lower())\n",
    "    return '; '.join(ts)\n",
    "\n",
    "def parse_json(j):\n",
    "\n",
    "    \n",
    "    try:\n",
    "        reference_url = j['href']\n",
    "            \n",
    "        j = j['fields']\n",
    "        #print(j)\n",
    "        \n",
    "        glide_id = None\n",
    "        disaster = j.get('disaster')\n",
    "        if disaster:\n",
    "            glide_id = disaster[0]['glide']\n",
    "    \n",
    "        \n",
    "        rec_id = j['id']\n",
    "        title = j['title']\n",
    "        original_text = j['body']\n",
    "        link_to_doc = j['url_alias']\n",
    "        file_url = j['file'][0]['url']\n",
    "        primary_country_iso3 = j['primary_country']['iso3']\n",
    "        primary_country = j['primary_country']['shortname']\n",
    "        author_org = j['source'][0]['shortname']\n",
    "        report_date = j['date']['original']\n",
    "        themes = extract_themes(j)\n",
    "\n",
    "        # 2 newlines will not necessarily reliably break out every paragraph\n",
    "        # but it's better than the alternative where some sentences will get cut in half\n",
    "        original_text_list = original_text.split(\"\\n\\n\")\n",
    "        idx_para = 0\n",
    "        for o in original_text_list:\n",
    "            if len(o) > 3:\n",
    "                row = ['situation report',reference_url,glide_id,rec_id,idx_para,primary_country,title,themes,o,file_url,o,author_org,report_date]\n",
    "                df_reliefweb_situation_report.loc[len(df_reliefweb_situation_report)] = row\n",
    "                idx_para += 1\n",
    "    except:\n",
    "        pass\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "193f3dbe-6939-436c-98ba-827d9ca2529e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#prep receiving df\n",
    "df_reliefweb_situation_report = pd.DataFrame(columns = ['record_type','source_url','glide_id','doc_id','idx_para','source_level_country','source_title','source_desc',\n",
    "                                                        'source_original_text','reference_url','text','authoring_org','reported_date'])\n",
    "\n",
    "situation_reports = get_rw_situation_reports(limit=500)\n",
    "\n",
    "for situation_report in situation_reports:\n",
    "    parse_json(situation_report)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c4937b0-bb00-4cab-80bf-be39de0d30e0",
   "metadata": {},
   "source": [
    "## Data Persistence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7fcd2ebf-2544-44e7-b327-2197ba2e9f4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df written to: stored_data\\rw_disaster_situation_reports_8a4af19ed04345f2b41d8de4adb98d4e.xlsx\n"
     ]
    }
   ],
   "source": [
    "if output_format == 'excel':\n",
    "    outfile = utils.write_to_excel(df_reliefweb_situation_report, filename=output_name)\n",
    "    print(f\"df written to: {outfile}\")\n",
    "elif output_format == 'postgres':\n",
    "    persist_to_postgres(db_conf, output_name, df_reliefweb_situation_report)\n",
    "else:\n",
    "    print(f\"unknown persistence type: {output_format}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f9790a9-61cf-402d-8656-5e3ca7f95772",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
