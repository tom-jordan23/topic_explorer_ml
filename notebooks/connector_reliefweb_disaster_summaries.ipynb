{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0e96a8f1-8453-4aa2-b4ed-74b6dac48210",
   "metadata": {},
   "source": [
    "## New 2/26/2024\n",
    "\n",
    "## This serves as the core logic to extracting content from reliefweb\n",
    "\n",
    "### Once this is finished, it replaces reliefweb_situation_reports\n",
    "\n",
    "## Process Steps\n",
    "1) get the last 500 disaster summaries from reliefweb\n",
    "2) process the json and build a dataframe structured such that:\n",
    "   1) each paragraph has its own row\n",
    "   2) each identified reference url in that paragraph is parsed out along with other metadata\n",
    "3) write resulting dataframe to postgres db - note this completely replaces the previously-existing table\n",
    "\n",
    "## Cautions\n",
    "Occasionally the reliefweb api call will return an error - to the effect that there's some bad chunk or something.\n",
    "I've left that unhandled. Conflicting evidence as to whether it's an intermittent issue, maybe cause by rate limiting,\n",
    " or an issue related to a specific summary's format. Both seem to have been the case at various times.\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a14080ca-4ca1-4280-9d02-c63b37654596",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "\n",
    "import json\n",
    "import os\n",
    "import uuid\n",
    "import re\n",
    "import importlib\n",
    "\n",
    "from datetime import datetime\n",
    "\n",
    "import sys\n",
    "sys.path.append('utilities')\n",
    "import basic_utilities as utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "93c0e496-d88d-47f0-8349-d69a60324fdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# key config vars\n",
    "api_endpoint = 'https://api.reliefweb.int/v1/disasters?appname=amcross'\n",
    "\n",
    "\n",
    "output_format = 'excel' #other option is 'postgres'\n",
    "output_name = 'rw_disaster_summaries'\n",
    "\n",
    "## If you plan to connect to a database\n",
    "db_conf ={\n",
    "    'host':\"xxx\",\n",
    "    'port':'5432',\n",
    "    'database':\"postgres\",\n",
    "    'user':\"postgres\",\n",
    "    'password':\"xxx\"\n",
    "}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5a9a81ba-1169-4abd-9ba9-5e02d2f4b3d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_rw_disaster_summaries(limit=500):\n",
    "    \n",
    "\n",
    "\n",
    "    #set a high limit for latest in case the job doesn't run for a long time\n",
    "    params = {\n",
    "    'appname': 'amcross','profile': 'full','preset': 'latest','limit': limit\n",
    "    #,'filter[field]': \"status\",'filter[value]':'ongoing'\n",
    "    ,'offset':0\n",
    "    }\n",
    "\n",
    "    # Make the API request\n",
    "    response = requests.get(api_endpoint, params=params)\n",
    "    \n",
    "    # Check the status of the response\n",
    "    if response.status_code == 200:\n",
    "        # Parse and use the response data (in JSON format)\n",
    "        data = response.json()\n",
    "        return data['data']\n",
    "        # for disasters if we don't return everything, can't get to the original api call\n",
    "        #return data\n",
    "    \n",
    "    else:\n",
    "        print(f\"Error: {response.status_code} - {response.text}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "\n",
    "   \n",
    "\n",
    "\n",
    "\n",
    "#get_rw_disaster_summaries(500)\n",
    "         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f6dcd39a-6381-4f67-981b-c346d3ff6550",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_summary_json(j):\n",
    "    reference_url = ''\n",
    "    file_url = ''\n",
    "    themes = []\n",
    "    author_org = ''\n",
    "    \n",
    "    rec_id = j['id']\n",
    "    j = j['fields']\n",
    "\n",
    "    status = j['status']\n",
    "    glide_id = j.get('glide')\n",
    "    status = j['status']\n",
    "    title = j['name']\n",
    "    description = j['description']\n",
    "    file_url = j['url_alias']\n",
    "    primary_country_iso3 = j['primary_country']['iso3']\n",
    "    primary_country = j['primary_country']['shortname']\n",
    "    report_date = j['date']['changed']\n",
    "\n",
    "    # 2 newlines will not necessarily reliably break out every paragraph\n",
    "    # but it's better than the alternative where some sentences will get cut in half\n",
    "    original_text_list = description.split(\"\\n\\n\")\n",
    "    idx_para=0\n",
    "    for o in original_text_list:\n",
    "        o = o.strip()\n",
    "        if len(o) > 3:\n",
    "            row = ['disaster summary',status,file_url,glide_id,rec_id,idx_para,primary_country,title,themes,o,reference_url,o,author_org,report_date]\n",
    "            df_rw_disaster_sum.loc[len(df_rw_disaster_sum)] = row\n",
    "            idx_para += 1\n",
    "\n",
    "\n",
    "def parse_references(text):\n",
    "    ref = ''\n",
    "    org = ''\n",
    "    date = ''\n",
    "    iso_date = None\n",
    "    url = ''\n",
    "    \n",
    "    pattern = r'\\([^)]+\\)\\)\\s?'  # Matches anything inside a ([xxxx](url)) pattern\n",
    "\n",
    "    ref = re.findall(pattern, text)\n",
    "    if len(ref) > 0:\n",
    "        last_reference = ref[-1]\n",
    "        #pattern = r'\\[(.*?)[,|.]\\s(\\d+\\s\\w+\\s\\d+)\\]\\((.*?)\\)'\n",
    "        pattern = r'\\[(.*?)[,|.]\\s?(\\d+\\s\\w+\\s\\d+)\\]\\((.*?)\\)'\n",
    "\n",
    "        # Use regex to find matches\n",
    "        matches = re.search(pattern, last_reference)\n",
    "        \n",
    "        if matches:\n",
    "            # Extracting the parts\n",
    "            org = matches.group(1)\n",
    "            date = matches.group(2)\n",
    "            url = matches.group(3)\n",
    "        \n",
    "\n",
    "            try:\n",
    "                date_object = datetime.strptime(date, \"%d %b %Y\") # 01 Jul 2023\n",
    "                iso_date = date_object.date().isoformat()\n",
    "\n",
    "            except:\n",
    "                try:\n",
    "                    date_object = datetime.strptime(date, \"%d %B %Y\") # 01 July 2023\n",
    "                    iso_date = date_object.date().isoformat()\n",
    "                except:\n",
    "                    pass\n",
    "\n",
    "            #date_object = datetime.strptime(date, \"%d %b %Y\")\n",
    "           \n",
    "            \n",
    "        else:\n",
    "            #TODO - some patterns don't get found... need to improve the pattern match\n",
    "            pass\n",
    "            #print(f\"No match found. {ref}\")\n",
    "\n",
    "    \n",
    "    return pd.Series({'references':ref,'auth_org':org,'date_str':date,'date_iso':iso_date,'reference_url':url})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1ddf6caa-4c2c-402d-8673-9e47209642f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare receiving df\n",
    "df_rw_disaster_sum = pd.DataFrame(columns = ['record_type','status','source_url','glide_id','doc_id','idx_para','source_level_country','source_title','source_desc',\n",
    "                                                        'source_original_text','reference_url','text','authoring_org','reported_date'])\n",
    "\n",
    "disaster_data = get_rw_disaster_summaries(limit=200)\n",
    "\n",
    "for summary in disaster_data:\n",
    "    parse_summary_json(summary)\n",
    "\n",
    "# now parse the idenfitied urls \n",
    "df_rw_disaster_sum['authoring_org'] = 'reliefweb'\n",
    "df_rw_disaster_sum[['references','reference_auth_org','reference_date_str','reference_date_iso','reference_url']] = df_rw_disaster_sum['text'].apply(parse_references) \n",
    "df_rw_disaster_sum['para_id'] = 'rwdisastersumm_' + df_rw_disaster_sum['glide_id'].apply(lambda x: x.lower()) + '_' + df_rw_disaster_sum['idx_para'].astype(str)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c4937b0-bb00-4cab-80bf-be39de0d30e0",
   "metadata": {},
   "source": [
    "## Data Persistence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "963a6fb4-2e50-451e-bce9-df4b387e401d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df written to: stored_data\\rw_disaster_summaries_64ae8b73482143a6b1361be7d3a124df.xlsx\n"
     ]
    }
   ],
   "source": [
    "\n",
    "if output_format == 'excel':\n",
    "    outfile = utils.write_to_excel(df_rw_disaster_sum, filename=output_name)\n",
    "    print(f\"df written to: {outfile}\")\n",
    "elif output_format == 'postgres':\n",
    "    persist_to_postgres(db_conf, output_name, df_rw_disaster_sum)\n",
    "else:\n",
    "    print(f\"unknown persistence type: {output_format}\")\n",
    "          \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "14e244fe-8964-4a4d-a595-1540a18895ab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1053, 18)"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_rw_disaster_sum.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0965745-739c-4235-85b4-a72881bddc8e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
