{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "e6fb2107-24f1-4355-9b1f-157915d02dad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy import displacy\n",
    "\n",
    "import re\n",
    "import pandas as pd\n",
    "from textacy import extract\n",
    "\n",
    "from collections import defaultdict \n",
    "from fuzzywuzzy import fuzz\n",
    "import time\n",
    "import uuid\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "62dddbe2-cc32-4ea6-90a4-e5d54c109b7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time.struct_time(tm_year=2023, tm_mon=12, tm_mday=2, tm_hour=8, tm_min=18, tm_sec=54, tm_wday=5, tm_yday=336, tm_isdst=0)\n"
     ]
    }
   ],
   "source": [
    "pd.set_option('display.max_columns', None)\n",
    "print(time.localtime())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "ebc9b065-6c1f-44ff-a53c-9a79e3f1a24c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#files\n",
    "pcode_file = \"D://projects//_external_files//cod_files//combined_locations//locations.csv\"\n",
    "situation_reports = \"D://projects//_external_files//surveyor//rw_siturep_preprocessed//reliefweb_situation_reports_c0e1868b386c4398a2f949ea652457b3.xlsx\"\n",
    "#situation_reports = \"D://projects//_external_files//surveyor//rw_disaster_preprocessed//disaster_summaries_b01b1f563f4d46ac91216604809f5903.xlsx\"\n",
    "\n",
    "#situation_reports = \"D:\\projects\\_external_files\\reliefweb_disaster_reports\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9b94269-2fda-432c-bd1d-1c5097f2be43",
   "metadata": {},
   "source": [
    "## Load geolocation_services\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "20cfd8aa-4f25-4857-abad-ba987499fe02",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_location = pd.read_csv(pcode_file)\n",
    "\n",
    "def get_pcode_from_location(loc, country_prefix='XX', lang_code='all'):\n",
    "\n",
    "    if country_prefix != 'XX': #if the country prefix is set, limit search to that\n",
    "        df_loc = df_location[df_location['pcode_prefix'] == country_prefix]\n",
    "    else:\n",
    "        df_loc = df_location\n",
    "\n",
    "    if lang_code != 'all': #secondary filter - especially important to remove dupes with diff langs share the same script\n",
    "        df_loc = df_loc[df_loc['lang_code'] == lang_code]\n",
    "        \n",
    "    matches = df_loc['pcode'][df_loc['location_name'].str.lower() == loc.lower()].tolist()\n",
    "\n",
    "    #if the match fails, try again on the normalized name\n",
    "    if len(matches) == 0:\n",
    "        #remove common variations in names that can cause misses\n",
    "        n_loc = re.sub(r'[^a-zA-Z]', '', loc)\n",
    "\n",
    "        #this will cause problems for non-English.. so if then len is 0, exit\n",
    "        if len(n_loc) == 0:\n",
    "            return []\n",
    "            \n",
    "        matches = df_loc['pcode'][df_loc['location_normalized'].str.lower() == n_loc.lower()].tolist()\n",
    "        \n",
    "\n",
    "    #now check results\n",
    "    if len(matches) > 1:\n",
    "        #print(f\"more than 1 matches... likely due to different granularity of entities with the same name (ie. Herat City in Herat Province) {matches}\")\n",
    "        #print(f\"returning the lowest granularity match. {min(matches, key=len)}\")\n",
    "        #print(\"if the pcodes are all the same granularity.... you get the first element.\")\n",
    "        return min(matches, key=len)\n",
    "            \n",
    "        return matches\n",
    "    elif len(matches) == 1:\n",
    "        return matches\n",
    "\n",
    "    else:\n",
    "        #couldn't find a match, do a fuzzy search\n",
    "        compare_list = list(set(df_loc['location_name'].tolist()))\n",
    "        possible_matches=[]\n",
    "        for i in compare_list:\n",
    "            if fuzz.ratio(loc,i) > 70:\n",
    "                possible_matches.append(i)\n",
    "                print (f\"No exact match to '{loc}'. see if these alternative spellings are correct: {possible_matches}\")\n",
    "\n",
    "    \n",
    "    return []\n",
    "\n",
    "assert get_pcode_from_location('istanbul')[0] == 'TUR034'\n",
    "\n",
    "def get_adm_lvl_from_pcode(pcode):\n",
    "    return list(set(df_location['adm_lvl'][df_location['pcode'] == pcode].tolist()))\n",
    "    \n",
    "def get_name_in_lang(pcode, lang='en'):\n",
    "    return list(set(df_location['location_name'][(df_location['pcode'] == pcode) & (df_location['lang_code'] == lang)].tolist()))\n",
    "\n",
    "def get_descendents_of(pcode, lang='en', include_self=True):\n",
    "    if include_self==True:\n",
    "        return df_location[df_location['pcode'].str.contains(pcode) & (df_location['lang_code'] == lang)]\n",
    "    else:\n",
    "        return df_location[df_location['pcode'].str.contains(pcode) & (df_location['lang_code'] == lang)\\\n",
    "        & (df_location['pcode'] != pcode)]\n",
    "\n",
    "def get_admin_chain(pcode, lang='en'):\n",
    "    split_pcode = df_location['split_pcode'][df_location['pcode'] == pcode].tolist()[0]\n",
    "    levels = split_pcode.split(\".\")\n",
    "    pc =''\n",
    "    admin_chain = []\n",
    "    #rebuild the pcode one level at a time\n",
    "    for i in levels:\n",
    "        pc = pc + i\n",
    "        admin_chain.append(df_location['location_name'][(df_location['pcode'] == pc) & (df_location['lang_code'] == lang)].tolist()[0])\n",
    "\n",
    "    return admin_chain\n",
    "\n",
    "def get_all_locations(lang_code='all'):\n",
    "\n",
    "    #return all unique location names\n",
    "    if lang_code == 'all':\n",
    "        return list(set(df_location['location_name'].to_list()))\n",
    "    else:\n",
    "        return list(set(df_location['location_name'][df_location['lang_code'] == lang_code].to_list()))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8734f6d-b090-454c-aad7-fda7d38d0b12",
   "metadata": {},
   "source": [
    "## Load Preprocessing Routines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "4bf238f1-4813-4690-9dcc-c7d9c704364f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def string_preprocess(text):\n",
    "    \n",
    "    def convert_spelled_nums_to_digit(token):\n",
    "        clean_token = re.sub(r'[^a-zA-Z]', '', token).lower()\n",
    "        \n",
    "        mappings = {\n",
    "            'one' : 1,'two' : 2,'three' : 3,'four' : 4,'five' : 5,'six' : 6,'seven' : 7,'eight' : 8,'nine' : 9, 'ten' : 10\n",
    "            ,'eleven' : 11, 'twelve' : 12, 'thirteen':13, 'fourteen':14, 'fifteen':15, 'sixteen':16, 'seventeen':17\n",
    "            ,'eighteen':18, 'nineteen':19, 'twenty':20, 'dozen':12\n",
    "        }\n",
    "    \n",
    "        if mappings.get(clean_token) is not None:\n",
    "            return mappings[clean_token]\n",
    "        else:\n",
    "            return token\n",
    "        \n",
    "    text = text.replace(\"\\n\",\" \")\n",
    "    text = text.replace(\"-\",\"_\") #replace so tokenization doesn't separate\n",
    "\n",
    "    #turn 'four' into 4\n",
    "    text = ' '.join([str(convert_spelled_nums_to_digit(t)) for t in text.split(\" \")])\n",
    "\n",
    "\n",
    "    # remove content in parentheses\n",
    "    #processed_string = re.sub(r'\\([^)]*\\)', '', input_string)\n",
    "\n",
    "    #remove all non alpha numeric and punctuation\n",
    "    pattern = r'[^a-zA-Z0-9\\s\\,\\.\\?\\!\\-\\(\\)]'\n",
    "    text = re.sub(pattern, '', text)\n",
    "\n",
    "    pattern = r'(\\d+)\\s+million'\n",
    "    text = re.sub(r'(\\d+)\\s+million', r'\\1,000,000', text)\n",
    "\n",
    "    \n",
    "    #remove commas that serve as thousands separators\n",
    "    #Hack... fix this so I don't have to run it 3x\n",
    "    text = re.sub(r'(\\d+),(\\d+)', r'\\1\\2', text)\n",
    "    text = re.sub(r'(\\d+),(\\d+)', r'\\1\\2', text)\n",
    "    text = re.sub(r'(\\d+),(\\d+)', r'\\1\\2', text)\n",
    "    text = text.replace(\"\\s+\",\"\\s\")\n",
    "    return text\n",
    "\n",
    "def string_remove_parenthetical_content(text):\n",
    "    # Use regular expression to remove content inside parentheses\n",
    "    text = re.sub(r'\\([^)]*\\)', '', text)\n",
    "    return text\n",
    "    \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff189ddf-4fc4-4d75-a881-0274b2b919ce",
   "metadata": {},
   "source": [
    "## Load NLP routines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "43e49e55-1a83-4914-803f-6b5cdb069a7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_md\")\n",
    "\n",
    "# Create patterns and add to the entity ruler to better find locations\n",
    "\n",
    "all_locs = get_all_locations(lang_code='en')\n",
    "gpes = []\n",
    "\n",
    "STOP_LOCS = ['of','can']\n",
    "all_locs = [e for e in all_locs if e.lower() not in STOP_LOCS]\n",
    "\n",
    "# create pattern rules for locations based on the COD files\n",
    "for l in all_locs:\n",
    "    token_sequence=[]\n",
    "    for token in l.split('\\s+'):\n",
    "        token_sequence.append({\"LOWER\":token.lower()})\n",
    "    x = {'label':'GPE', 'pattern': token_sequence, 'id':get_pcode_from_location(l, lang_code='en')[0]}\n",
    "    gpes.append(x)\n",
    "    #print(get_pcode_from_location(l, lang_code='en'))\n",
    "\n",
    "ruler = nlp.add_pipe('entity_ruler', before='ner')\n",
    "ruler.add_patterns(gpes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "e8591a05-cefd-41b9-800f-4f9ac23b7358",
   "metadata": {},
   "outputs": [],
   "source": [
    "#keyword_indicators\n",
    "indicators = {\n",
    "    'i_people' : ['people','person','child','man','woman','civilian','colleague','fatality','individual']\n",
    "    ,'i_killed' : ['dead','fatal','die','kill','deceased','fatality','fatality'] #think about how to incorporate 2 co-existing terms \"648 people who lost their lives\"\n",
    "    ,'i_injured' : ['injure','wound','wounded']\n",
    "    ,'i_damage' : ['damage','destroy','collapse']\n",
    "    ,'i_health_infrastructure' : ['hospital','surgery']\n",
    "    ,'i_education_infrastructure' : ['school','university']\n",
    "    ,'i_cash_xfer' : ['xx']\n",
    "    ,'i_wash' : ['sanitation','water','sewer','drain','drainage']\n",
    "    ,'i_shelter' : ['shelter','tent','camp','blanket']\n",
    "    ,'i_food' : ['food','cook','stove','feed','feed','nutrient','meal']\n",
    "    ,'i_health' : ['health','medical','medicine']\n",
    "    ,'i_gender_vuln' : ['dignity','gender','pregnant','lactate','lactating']\n",
    "    ,'i_protection' : ['trauma','mental']\n",
    "    ,'i_response_capacity' : ['personnel']\n",
    "    ,'i_other_infrastructure' : ['communicate','radio','internet','telecommunication','electric','line']\n",
    "    ,'i_money' : ['grant','loan','finance','appeal','chf','fund']\n",
    "    ,'i_other' : ['biometric']\n",
    "    ,'i_problem' : ['challenge']\n",
    "    ,'i_demand_side' : ['need','demand','gap','priority', 'receive'] # note receive implies both supply and demand\n",
    "    ,'i_supply_side' : ['response','contribute','provide','source','address','deploy','receive'] # note receive implies both supply and demand\n",
    "\n",
    "    ,'i_assessments' : ['assess','assessment']\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "5f589028-1219-48ce-9e27-283ce730511a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_future_tense_verb(doc):\n",
    "    def is_future_tense(token):\n",
    "        #Check if a token is indicative of future tense.\n",
    "        return (\n",
    "            token.tag_ == \"MD\" and token.text.lower() == \"will\"\n",
    "            or (token.dep_ == \"aux\" and token.head.lemma_ == \"will\")\n",
    "        )\n",
    "\n",
    "    for t in doc:\n",
    "        if is_future_tense(t):\n",
    "            return f\"{t.text} {t.head}\"\n",
    "\n",
    "def find_and_add_indicator(df, indicators):\n",
    "    ind_counter = []\n",
    "    for ind in indicators:\n",
    "  \n",
    "        df[ind] = df['lower_lemmas'].apply(lambda x: 1 if len([w for w in x if w in indicators[ind]])>0 else 0)\n",
    "        ind_counter.append(ind)\n",
    "        #print(ind_counter)\n",
    "    df['i_count'] = df[ind_counter].sum(axis=1)\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def declare_primary_record_type(row):\n",
    "\n",
    "    if row['i_count'] == 0:\n",
    "        return 'background'\n",
    "    elif row['i_supply_side']:\n",
    "        return 'response_details'\n",
    "    elif row['i_demand_side']:\n",
    "        return 'demand_side'\n",
    "    elif row[['i_damage','i_health_infrastructure','i_education_infrastructure']].sum() > 0:\n",
    "        return 'damage_to_homes_and_infrastructure'\n",
    "    else:\n",
    "        return 'other'\n",
    "\n",
    "def obtain_killed_numeric_value(doc):\n",
    "\n",
    "    key_values = []\n",
    "    just_count = []\n",
    "    \n",
    "    def check_flags(lst):\n",
    "        for l in lst:\n",
    "            if l == -1:\n",
    "                return False\n",
    "        return True\n",
    "\n",
    "    def reset_indicators():\n",
    "        return -1, -1, -1\n",
    "\n",
    "    noun, attribute, count = reset_indicators()\n",
    "            \n",
    "    for t in doc:\n",
    "        if (str(t).isdigit()) & (t.ent_type_ not in ['DATE','TIME']):\n",
    "            count = t\n",
    "        if (t.lemma_ in indicators['i_people']) or (t.ent_type_ == 'NORP'):\n",
    "            noun = t\n",
    "        if t.lemma_ in indicators['i_killed']:\n",
    "            attribute = t\n",
    "        if check_flags([noun,attribute,count]):\n",
    "\n",
    "            noun_att_cnt = (noun,attribute,count)\n",
    "            key_values.append(noun_att_cnt)\n",
    "            just_count.append(count)\n",
    "\n",
    "            noun, attribute, count = reset_indicators()\n",
    "\n",
    "    #if more than 1 figure is returned, typically those will be\n",
    "    #contextualizing numbers, just return the first\n",
    "    if len(just_count) > 0:\n",
    "        return just_count[0]\n",
    "\n",
    "\n",
    "def obtain_injured_numeric_value(doc):\n",
    "\n",
    "    key_values = []\n",
    "    just_count = []\n",
    "    \n",
    "    def check_flags(lst):\n",
    "        for l in lst:\n",
    "            if l == -1:\n",
    "                return False\n",
    "        return True\n",
    "\n",
    "    def reset_indicators():\n",
    "        return -1, -1, -1\n",
    "\n",
    "    noun, attribute, count = reset_indicators()\n",
    "            \n",
    "    for t in doc:\n",
    "        if (str(t).isdigit()) & (t.ent_type_ not in ['DATE','TIME']):\n",
    "            count = t\n",
    "        if (t.lemma_ in indicators['i_people']) or (t.ent_type_ == 'NORP'):\n",
    "            noun = t\n",
    "        if t.lemma_ in indicators['i_injured']:\n",
    "            attribute = t\n",
    "        if check_flags([noun,attribute,count]):\n",
    "\n",
    "            noun_att_cnt = (noun,attribute,count)\n",
    "            key_values.append(noun_att_cnt)\n",
    "            just_count.append(count)\n",
    "\n",
    "            noun, attribute, count = reset_indicators()\n",
    "\n",
    "    #if more than 1 figure is returned, typically those will be\n",
    "    #contextualizing numbers, just return the first\n",
    "    if len(just_count) > 0:\n",
    "        return just_count[0]\n",
    "\n",
    "\n",
    "def OLD_obtain_killed_numeric_value(doc):\n",
    "\n",
    "    key_values = []\n",
    "    just_count = []\n",
    "    \n",
    "    def check_flags(lst):\n",
    "        for l in lst:\n",
    "            if l == -1:\n",
    "                return False\n",
    "        return True\n",
    "\n",
    "    #doc = doc.tolist()[0]\n",
    "    attribute = -1\n",
    "    noun = -1\n",
    "    count = -1\n",
    "\n",
    "    for t in doc:\n",
    "        if (str(t).isdigit()) & (t.ent_type_ not in ['DATE','TIME']):\n",
    "            count = t\n",
    "        if (t.lemma_ in indicators['i_people']) or (t.ent_type_ == 'NORP'):\n",
    "            noun = t\n",
    "        if t.lemma_ in indicators['i_killed']:\n",
    "            attribute = t\n",
    "\n",
    "        if check_flags([noun,attribute,count]):\n",
    "\n",
    "            noun_att_cnt = (noun,attribute,count)\n",
    "            key_values.append(noun_att_cnt)\n",
    "            just_count.append(count)\n",
    "\n",
    "            noun = -1\n",
    "            attribute = -1\n",
    "            count = -1\n",
    "\n",
    "    #changing to return only the count\n",
    "    return just_count\n",
    "    #return key_values\n",
    "            \n",
    "    \n",
    "def OLD_obtain_injured_numeric_value(doc):\n",
    "\n",
    "    key_values = []\n",
    "    just_count = []\n",
    "    \n",
    "    def check_flags(lst):\n",
    "        for l in lst:\n",
    "            if l == -1:\n",
    "                return False\n",
    "        return True\n",
    "\n",
    "    #doc = doc.tolist()[0]\n",
    "    attribute = -1\n",
    "    noun = -1\n",
    "    count = -1\n",
    "\n",
    "    for t in doc:\n",
    "        if (str(t).isdigit()) & (t.ent_type_ not in ['DATE','TIME']):\n",
    "            count = t\n",
    "        if (t.lemma_ in indicators['i_people']) or (t.ent_type_ == 'NORP'):\n",
    "            noun = t\n",
    "        if t.lemma_ in indicators['i_injured']:\n",
    "            attribute = t\n",
    "\n",
    "        if check_flags([noun,attribute,count]):\n",
    "\n",
    "            noun_att_cnt = (noun,attribute,count)\n",
    "            key_values.append(noun_att_cnt)\n",
    "            just_count.append(count)\n",
    "\n",
    "            noun = -1\n",
    "            attribute = -1\n",
    "            count = -1\n",
    "\n",
    "    #changing to return only the count\n",
    "    return just_count\n",
    "    #return key_values\n",
    "\n",
    "def obtain_counted_noun_chunks(doc):\n",
    "    counted_things = []\n",
    "    for x in list(extract.noun_chunks(doc)):\n",
    "        for token in x:\n",
    "            if str(token).isdigit():\n",
    "                counted_things.append(x)\n",
    "                continue\n",
    "    if len(counted_things) > 0:\n",
    "        return counted_things\n",
    "    else:\n",
    "        return ''\n",
    "\n",
    "\n",
    "\n",
    "def obtain_all_entities(doc):\n",
    "\n",
    "    STOP_ENTS = ['WASH','PSS','GTC','PFA','NFI','IYCF']\n",
    "    STOP_ENT_TYPE = ['CARDINAL','ORDINAL']\n",
    "    #stop_ents = STOP_ENTS\n",
    "    ents = list(extract.entities(doc))\n",
    "    if len(ents) < 1:\n",
    "        return None\n",
    "    entities = defaultdict(list) \n",
    "    for e in ents:\n",
    "        #if e.text not in stop_ents:\n",
    "        if e.label_ not in STOP_ENT_TYPE:\n",
    "            entities[e.label_].append(e)\n",
    "\n",
    "    return entities   \n",
    "\n",
    "def extract_entities(row):\n",
    "    entities = row['entities']\n",
    "    if entities is None:\n",
    "        return ''\n",
    "    en=[]\n",
    "    for label in entities:\n",
    "        for e in entities.get(label):\n",
    "            ent = ' '.join([w.text for w in e]).strip()\n",
    "            en.append([label,ent])\n",
    "            \n",
    "    return en\n",
    "\n",
    "\n",
    "def extract_ncs(row):\n",
    "    \n",
    "    #data type, list of spans\n",
    "    xs = row['noun_chunks']\n",
    "    if xs is None:\n",
    "        return ''\n",
    "    en=[]\n",
    "\n",
    "    for e in xs:\n",
    "        ent = ' '.join([w.text for w in e]).strip()\n",
    "        en.append(['NOUN_CHUNK',ent])\n",
    "    return en\n",
    "\n",
    "\n",
    "def extract_numeric_key_values(row):\n",
    "    #data type, list of spans\n",
    "    xs = row['num_others']\n",
    "    if xs is None:\n",
    "        return ''\n",
    "    return_list=[]\n",
    "\n",
    "    for e in xs:\n",
    "        prefix = ''\n",
    "        numeric = ''\n",
    "        suffix = ''\n",
    "\n",
    "        for token in e:\n",
    "            if token.is_alpha == False:\n",
    "                numeric = token.text\n",
    "            elif numeric == '': #alpha but numeric not set yet, this is prefix\n",
    "                prefix = prefix + ' ' + token.text\n",
    "            else:\n",
    "                suffix = suffix + ' ' + token.text\n",
    "        \n",
    "        return_list.append([prefix.strip(),numeric,suffix.strip()])   \n",
    "        \n",
    "    return return_list\n",
    "\n",
    "\n",
    "def split_key_value_in_df(field,delim=','):\n",
    "\n",
    "    s = pd.Series({'prefix' : field, 'left_label' : field, 'right_label' : field})\n",
    "    \n",
    "    if isinstance(field, list):\n",
    "        fields = field\n",
    "    elif isinstance(field, str):\n",
    "        fields = field.split(delim)\n",
    "    else:\n",
    "        print(field)\n",
    "    \n",
    "     \n",
    "    if len(fields) == 2:\n",
    "        s = pd.Series({'prefix' : '', 'left_label' : fields[0], 'right_label' : fields[1]})\n",
    "    elif len(fields) == 3:\n",
    "        s = pd.Series({'prefix' : fields[0], 'left_label' : fields[1], 'right_label' : fields[2]})\n",
    "\n",
    "\n",
    "\n",
    "    return s\n",
    "\n",
    "def split_key_value_in_df_orig(field,left_label=\"d\",right_label=\"f\",delim=','):\n",
    "\n",
    "    s = pd.Series({left_label : field, right_label : field})\n",
    "    \n",
    "    if isinstance(field, list):\n",
    "     \n",
    "        if len(field) == 2:\n",
    "            s = pd.Series({left_label : field[0], right_label : field[1]})\n",
    "\n",
    "    elif isinstance(field, str):\n",
    "        fields = field.split(delim)\n",
    "        if len(fields) == 2:\n",
    "            s = pd.Series({left_label : fields[0], right_label : fields[1]})\n",
    "\n",
    "    return s"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e021d782-2fbb-4db6-afe1-913195073afa",
   "metadata": {},
   "source": [
    "## Now build base DF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "6183b2b6-a15f-4da2-b6c6-b8f17e4824d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>idx_sent</th>\n",
       "      <th>record_type</th>\n",
       "      <th>source_url</th>\n",
       "      <th>glide_id</th>\n",
       "      <th>idx_para</th>\n",
       "      <th>source_level_country</th>\n",
       "      <th>source_title</th>\n",
       "      <th>source_desc</th>\n",
       "      <th>source_original_text</th>\n",
       "      <th>reference_url</th>\n",
       "      <th>text</th>\n",
       "      <th>authoring_org</th>\n",
       "      <th>reported_date</th>\n",
       "      <th>string_sentence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>situation report</td>\n",
       "      <td>https://api.reliefweb.int/v1/reports/3942412</td>\n",
       "      <td>TC-2023-000030-VUT</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Vanuatu</td>\n",
       "      <td>Government of the Republic of Vanuatu: Nationa...</td>\n",
       "      <td>coordination; education; food and nutrition; h...</td>\n",
       "      <td>**Situation Overview**</td>\n",
       "      <td>https://reliefweb.int/attachments/e8ee305d-b72...</td>\n",
       "      <td>**Situation Overview**</td>\n",
       "      <td>Govt. Vanuatu</td>\n",
       "      <td>2023-03-11T00:00:00+00:00</td>\n",
       "      <td>Situation Overview.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>situation report</td>\n",
       "      <td>https://api.reliefweb.int/v1/reports/3942412</td>\n",
       "      <td>TC-2023-000030-VUT</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Vanuatu</td>\n",
       "      <td>Government of the Republic of Vanuatu: Nationa...</td>\n",
       "      <td>coordination; education; food and nutrition; h...</td>\n",
       "      <td>Rapid Assessment teams to impacted priority ar...</td>\n",
       "      <td>https://reliefweb.int/attachments/e8ee305d-b72...</td>\n",
       "      <td>Rapid Assessment teams to impacted priority ar...</td>\n",
       "      <td>Govt. Vanuatu</td>\n",
       "      <td>2023-03-11T00:00:00+00:00</td>\n",
       "      <td>Rapid Assessment teams to impacted priority ar...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>situation report</td>\n",
       "      <td>https://api.reliefweb.int/v1/reports/3942412</td>\n",
       "      <td>TC-2023-000030-VUT</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Vanuatu</td>\n",
       "      <td>Government of the Republic of Vanuatu: Nationa...</td>\n",
       "      <td>coordination; education; food and nutrition; h...</td>\n",
       "      <td>Rapid Assessment teams to impacted priority ar...</td>\n",
       "      <td>https://reliefweb.int/attachments/e8ee305d-b72...</td>\n",
       "      <td>Rapid Assessment teams to impacted priority ar...</td>\n",
       "      <td>Govt. Vanuatu</td>\n",
       "      <td>2023-03-11T00:00:00+00:00</td>\n",
       "      <td>NFI distribution to Shefa and Tafea with suppo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>situation report</td>\n",
       "      <td>https://api.reliefweb.int/v1/reports/3942412</td>\n",
       "      <td>TC-2023-000030-VUT</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Vanuatu</td>\n",
       "      <td>Government of the Republic of Vanuatu: Nationa...</td>\n",
       "      <td>coordination; education; food and nutrition; h...</td>\n",
       "      <td>Rapid Assessment teams to impacted priority ar...</td>\n",
       "      <td>https://reliefweb.int/attachments/e8ee305d-b72...</td>\n",
       "      <td>Rapid Assessment teams to impacted priority ar...</td>\n",
       "      <td>Govt. Vanuatu</td>\n",
       "      <td>2023-03-11T00:00:00+00:00</td>\n",
       "      <td>.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>situation report</td>\n",
       "      <td>https://api.reliefweb.int/v1/reports/3942412</td>\n",
       "      <td>TC-2023-000030-VUT</td>\n",
       "      <td>2.0</td>\n",
       "      <td>Vanuatu</td>\n",
       "      <td>Government of the Republic of Vanuatu: Nationa...</td>\n",
       "      <td>coordination; education; food and nutrition; h...</td>\n",
       "      <td>**Cluster Updates**</td>\n",
       "      <td>https://reliefweb.int/attachments/e8ee305d-b72...</td>\n",
       "      <td>**Cluster Updates**</td>\n",
       "      <td>Govt. Vanuatu</td>\n",
       "      <td>2023-03-11T00:00:00+00:00</td>\n",
       "      <td>Cluster Updates.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>situation report</td>\n",
       "      <td>https://api.reliefweb.int/v1/reports/3942412</td>\n",
       "      <td>TC-2023-000030-VUT</td>\n",
       "      <td>3.0</td>\n",
       "      <td>Vanuatu</td>\n",
       "      <td>Government of the Republic of Vanuatu: Nationa...</td>\n",
       "      <td>coordination; education; food and nutrition; h...</td>\n",
       "      <td>**Gender &amp; Protection Cluster**</td>\n",
       "      <td>https://reliefweb.int/attachments/e8ee305d-b72...</td>\n",
       "      <td>**Gender &amp; Protection Cluster**</td>\n",
       "      <td>Govt. Vanuatu</td>\n",
       "      <td>2023-03-11T00:00:00+00:00</td>\n",
       "      <td>Gender  Protection Cluster.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   idx_sent       record_type                                    source_url  \\\n",
       "0         0  situation report  https://api.reliefweb.int/v1/reports/3942412   \n",
       "1         1  situation report  https://api.reliefweb.int/v1/reports/3942412   \n",
       "2         2  situation report  https://api.reliefweb.int/v1/reports/3942412   \n",
       "3         3  situation report  https://api.reliefweb.int/v1/reports/3942412   \n",
       "4         4  situation report  https://api.reliefweb.int/v1/reports/3942412   \n",
       "5         5  situation report  https://api.reliefweb.int/v1/reports/3942412   \n",
       "\n",
       "             glide_id  idx_para source_level_country  \\\n",
       "0  TC-2023-000030-VUT       0.0              Vanuatu   \n",
       "1  TC-2023-000030-VUT       1.0              Vanuatu   \n",
       "2  TC-2023-000030-VUT       1.0              Vanuatu   \n",
       "3  TC-2023-000030-VUT       1.0              Vanuatu   \n",
       "4  TC-2023-000030-VUT       2.0              Vanuatu   \n",
       "5  TC-2023-000030-VUT       3.0              Vanuatu   \n",
       "\n",
       "                                        source_title  \\\n",
       "0  Government of the Republic of Vanuatu: Nationa...   \n",
       "1  Government of the Republic of Vanuatu: Nationa...   \n",
       "2  Government of the Republic of Vanuatu: Nationa...   \n",
       "3  Government of the Republic of Vanuatu: Nationa...   \n",
       "4  Government of the Republic of Vanuatu: Nationa...   \n",
       "5  Government of the Republic of Vanuatu: Nationa...   \n",
       "\n",
       "                                         source_desc  \\\n",
       "0  coordination; education; food and nutrition; h...   \n",
       "1  coordination; education; food and nutrition; h...   \n",
       "2  coordination; education; food and nutrition; h...   \n",
       "3  coordination; education; food and nutrition; h...   \n",
       "4  coordination; education; food and nutrition; h...   \n",
       "5  coordination; education; food and nutrition; h...   \n",
       "\n",
       "                                source_original_text  \\\n",
       "0                             **Situation Overview**   \n",
       "1  Rapid Assessment teams to impacted priority ar...   \n",
       "2  Rapid Assessment teams to impacted priority ar...   \n",
       "3  Rapid Assessment teams to impacted priority ar...   \n",
       "4                               **Cluster Updates**    \n",
       "5                    **Gender & Protection Cluster**   \n",
       "\n",
       "                                       reference_url  \\\n",
       "0  https://reliefweb.int/attachments/e8ee305d-b72...   \n",
       "1  https://reliefweb.int/attachments/e8ee305d-b72...   \n",
       "2  https://reliefweb.int/attachments/e8ee305d-b72...   \n",
       "3  https://reliefweb.int/attachments/e8ee305d-b72...   \n",
       "4  https://reliefweb.int/attachments/e8ee305d-b72...   \n",
       "5  https://reliefweb.int/attachments/e8ee305d-b72...   \n",
       "\n",
       "                                                text  authoring_org  \\\n",
       "0                             **Situation Overview**  Govt. Vanuatu   \n",
       "1  Rapid Assessment teams to impacted priority ar...  Govt. Vanuatu   \n",
       "2  Rapid Assessment teams to impacted priority ar...  Govt. Vanuatu   \n",
       "3  Rapid Assessment teams to impacted priority ar...  Govt. Vanuatu   \n",
       "4                               **Cluster Updates**   Govt. Vanuatu   \n",
       "5                    **Gender & Protection Cluster**  Govt. Vanuatu   \n",
       "\n",
       "               reported_date  \\\n",
       "0  2023-03-11T00:00:00+00:00   \n",
       "1  2023-03-11T00:00:00+00:00   \n",
       "2  2023-03-11T00:00:00+00:00   \n",
       "3  2023-03-11T00:00:00+00:00   \n",
       "4  2023-03-11T00:00:00+00:00   \n",
       "5  2023-03-11T00:00:00+00:00   \n",
       "\n",
       "                                     string_sentence  \n",
       "0                                Situation Overview.  \n",
       "1  Rapid Assessment teams to impacted priority ar...  \n",
       "2  NFI distribution to Shefa and Tafea with suppo...  \n",
       "3                                                  .  \n",
       "4                                   Cluster Updates.  \n",
       "5                        Gender  Protection Cluster.  "
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_excel(situation_reports)\n",
    "\n",
    "## Naive sentence split below is a bad idea....\n",
    "df['string_sentence'] = df['text'].astype(str).apply(lambda x: string_preprocess(x).split('.'))\n",
    "df = df.explode('string_sentence')\n",
    "df['string_sentence'] = df['string_sentence'].apply(lambda x: x.strip() + '.')\n",
    "df = df.reset_index(drop=True)\n",
    "\n",
    "#cheap way to index the sentences\n",
    "df = df.reset_index()\n",
    "df.rename(columns={'index':'idx_sent'}, inplace=True)\n",
    "df.head(6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "5e67eccd-1929-4b85-92b5-a419e39596a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- A package of integrated minimum health services for mothers and children staying at evacuation centres has been defined. Logistics arrangements are ongoing for initiation of service delivery on March 13th.\n",
      "\n",
      "Logistics arrangements are ongoing for initiation of service delivery on March 13th.\n"
     ]
    }
   ],
   "source": [
    "idx = df.sample(1).index[0]\n",
    "print(df.loc[idx]['source_original_text']) #.tolist()[0]\n",
    "print()\n",
    "print(df.loc[idx]['string_sentence']) #.tolist()[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "2173564f-a0b3-4856-8d9b-da2bb23a1481",
   "metadata": {},
   "outputs": [],
   "source": [
    "#b build out initial dataframe\n",
    "df['spacy_doc'] = df['string_sentence'].apply(lambda x: nlp(x))\n",
    "df['lower_lemmas'] = df['spacy_doc'].apply(lambda x: [w.lemma_.lower() for w in x])\n",
    "\n",
    "df['string_sent_wo_parens'] = df['string_sentence'].apply(string_remove_parenthetical_content)\n",
    "df['spacy_wo_parens'] = df['string_sent_wo_parens'].apply(lambda x: nlp(x))\n",
    "df['wo_parens_lower_lemmas'] = df['spacy_wo_parens'].apply(lambda x: [w.lemma_.lower() for w in x])\n",
    "df['locations'] = df['spacy_doc'].apply(lambda doc: [e.text for e in doc.ents if e.label_ == 'GPE'])\n",
    "df['dates'] = df['spacy_doc'].apply(lambda doc: [e.text for e in doc.ents if e.label_ == 'DATE'])\n",
    "df['svot'] = df['spacy_wo_parens'].apply(lambda doc: list(extract.subject_verb_object_triples(doc)))\n",
    "df['future_verbs'] = df['spacy_doc'].apply(get_future_tense_verb)\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "a008d4d5-e16c-44c7-b277-c06ac4fb7763",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = find_and_add_indicator(df, indicators)\n",
    "df['record_type'] = df.apply(declare_primary_record_type, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "298fe78b-a2b8-4b98-beb5-9358c3cf0d19",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['num_killed'] = df['spacy_wo_parens'][df['i_killed'] == 1].apply(obtain_killed_numeric_value)\n",
    "df['num_injured'] = df['spacy_wo_parens'][df['i_injured'] == 1].apply(obtain_injured_numeric_value)\n",
    "#df['num_killed'] = df['wo_parens_lower_lemmas'][df['i_killed'] == 1].apply(obtain_killed_numeric_value)\n",
    "#df['num_injured'] = df['wo_parens_lower_lemmas'][df['i_injured'] == 1].apply(obtain_injured_numeric_value)\n",
    "\n",
    "df['num_others'] = df['spacy_wo_parens'].apply(obtain_counted_noun_chunks)\n",
    "\n",
    "stop_noun_chunks = ['which','these','that','it','this']\n",
    "df['noun_chunks'] = df['spacy_wo_parens'].apply(lambda doc: [i for i in list(extract.noun_chunks(doc)) if i.text.lower() not in stop_noun_chunks])\n",
    "df['entities'] = df['spacy_wo_parens'].apply(obtain_all_entities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "1b13c0f9-6439-4e74-9ec2-1a8f39ca18c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_uuid(x):\n",
    "    foo = uuid.uuid4().hex\n",
    "    return foo\n",
    "    \n",
    "df['sent_idx'] = df['string_sentence'].apply(generate_uuid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "a5127499-47cc-4fbd-b701-8f25dcd6e1f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df.to_csv(\"c://temp//foo.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "742feaf2-ca61-4a72-8337-d369b06cf3bf",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#use this as a repeatable-ish pattern for expanding on all the qualitative fields\n",
    "df_entities = df[['source_url','sent_idx','string_sentence','entities']][df['entities'].isna() == False].copy()\n",
    "df_entities['tmp'] = df_entities.apply(extract_entities, axis=1)\n",
    "df_entities = df_entities.drop(columns=['entities'])\n",
    "df_entities = df_entities.explode('tmp')\n",
    "df_entities['rec_type'] = 'ENTITY'\n",
    "df_entities = df_entities[df_entities['tmp'].isna() == False].copy() # added as a final cleanup to resolve stop ents\n",
    "df_entities[['rec_prefix','rec_key','rec_value']] = df_entities.apply(lambda x: split_key_value_in_df(x.tmp), axis=1)\n",
    "\n",
    "#now noun_chunks\n",
    "df_nouns = df[['source_url','sent_idx','string_sentence','noun_chunks']][df['noun_chunks'].isna() == False].copy()\n",
    "df_nouns['tmp'] = df_nouns.apply(extract_ncs, axis=1)\n",
    "df_nouns = df_nouns.drop(columns=['noun_chunks'])\n",
    "df_nouns = df_nouns.explode('tmp')\n",
    "df_nouns['rec_type'] = 'NOUN_SEQUENCE'\n",
    "df_nouns = df_nouns[df_nouns['tmp'].isna() == False].copy()\n",
    "df_nouns[['rec_prefix','rec_key','rec_value']] = df_nouns.apply(lambda x: split_key_value_in_df(x.tmp), axis=1)\n",
    "\n",
    "#quantitative values\n",
    "df_quants = df[['source_url','sent_idx','string_sentence','num_others']][df['num_others'] != ''].copy()\n",
    "df_quants['tmp'] = df_quants.apply(extract_numeric_key_values, axis=1)\n",
    "df_quants = df_quants.drop(columns=['num_others'])\n",
    "df_quants = df_quants.explode('tmp')\n",
    "df_quants['rec_type'] = 'QUANTIFIED_NOUN'\n",
    "df_quants[['rec_prefix','rec_key','rec_value']] = df_quants.apply(lambda x: split_key_value_in_df(x.tmp), axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "cd7c520e-e9a4-4926-b6f2-a9e704056638",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>source_url</th>\n",
       "      <th>sent_idx</th>\n",
       "      <th>string_sentence</th>\n",
       "      <th>rec_type</th>\n",
       "      <th>rec_prefix</th>\n",
       "      <th>rec_key</th>\n",
       "      <th>rec_value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>https://api.reliefweb.int/v1/reports/3942412</td>\n",
       "      <td>c542bbd546f24a8fadef06c44e00ee75</td>\n",
       "      <td>GPC working with AA Epau to distribute 120 dig...</td>\n",
       "      <td>QUANTIFIED_NOUN</td>\n",
       "      <td></td>\n",
       "      <td>120</td>\n",
       "      <td>dignity kits</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>https://api.reliefweb.int/v1/reports/3942412</td>\n",
       "      <td>0399d1bd2ee646e998630bbd0f21d05f</td>\n",
       "      <td>48 dignity kits being distributed on Mataso is...</td>\n",
       "      <td>QUANTIFIED_NOUN</td>\n",
       "      <td></td>\n",
       "      <td>48</td>\n",
       "      <td>dignity kits</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>https://api.reliefweb.int/v1/reports/3942412</td>\n",
       "      <td>840d571fff7d4a8aab6c0b914c715c48</td>\n",
       "      <td>Disability Inclusion in Emergencies Subcluster...</td>\n",
       "      <td>QUANTIFIED_NOUN</td>\n",
       "      <td></td>\n",
       "      <td>13</td>\n",
       "      <td>March</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>https://api.reliefweb.int/v1/reports/3942412</td>\n",
       "      <td>7006cc561a92471a9ff7ea232bb321cf</td>\n",
       "      <td>Inactive EC Greater Port Vila  9, Rural Efate.</td>\n",
       "      <td>QUANTIFIED_NOUN</td>\n",
       "      <td>Inactive EC Greater Port Vila</td>\n",
       "      <td>,</td>\n",
       "      <td>Rural Efate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>https://api.reliefweb.int/v1/reports/3942412</td>\n",
       "      <td>f4fca5f0152249d09c0e2209288af81f</td>\n",
       "      <td>Procurement of the dry rations is underway for...</td>\n",
       "      <td>QUANTIFIED_NOUN</td>\n",
       "      <td></td>\n",
       "      <td>2</td>\n",
       "      <td>and areas</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59952</th>\n",
       "      <td>https://api.reliefweb.int/v1/reports/4018475</td>\n",
       "      <td>3074e8d78ac941f0b5cd75e020a35b58</td>\n",
       "      <td>The forecasting results will be updated monthl...</td>\n",
       "      <td>ENTITY</td>\n",
       "      <td></td>\n",
       "      <td>DATE</td>\n",
       "      <td>2024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59952</th>\n",
       "      <td>https://api.reliefweb.int/v1/reports/4018475</td>\n",
       "      <td>3074e8d78ac941f0b5cd75e020a35b58</td>\n",
       "      <td>The forecasting results will be updated monthl...</td>\n",
       "      <td>ENTITY</td>\n",
       "      <td></td>\n",
       "      <td>ORG</td>\n",
       "      <td>Mekong Delta</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59954</th>\n",
       "      <td>https://api.reliefweb.int/v1/reports/4018475</td>\n",
       "      <td>5b9162f2bfbd453fa5e9946b82426b16</td>\n",
       "      <td>The flow volume of the Tonle Sap Lake was slig...</td>\n",
       "      <td>ENTITY</td>\n",
       "      <td></td>\n",
       "      <td>LOC</td>\n",
       "      <td>Tonle Sap Lake</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59954</th>\n",
       "      <td>https://api.reliefweb.int/v1/reports/4018475</td>\n",
       "      <td>5b9162f2bfbd453fa5e9946b82426b16</td>\n",
       "      <td>The flow volume of the Tonle Sap Lake was slig...</td>\n",
       "      <td>ENTITY</td>\n",
       "      <td></td>\n",
       "      <td>ORG</td>\n",
       "      <td>LTA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59954</th>\n",
       "      <td>https://api.reliefweb.int/v1/reports/4018475</td>\n",
       "      <td>5b9162f2bfbd453fa5e9946b82426b16</td>\n",
       "      <td>The flow volume of the Tonle Sap Lake was slig...</td>\n",
       "      <td>ENTITY</td>\n",
       "      <td></td>\n",
       "      <td>DATE</td>\n",
       "      <td>November 2023</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>328689 rows Ã— 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         source_url  \\\n",
       "6      https://api.reliefweb.int/v1/reports/3942412   \n",
       "8      https://api.reliefweb.int/v1/reports/3942412   \n",
       "10     https://api.reliefweb.int/v1/reports/3942412   \n",
       "23     https://api.reliefweb.int/v1/reports/3942412   \n",
       "25     https://api.reliefweb.int/v1/reports/3942412   \n",
       "...                                             ...   \n",
       "59952  https://api.reliefweb.int/v1/reports/4018475   \n",
       "59952  https://api.reliefweb.int/v1/reports/4018475   \n",
       "59954  https://api.reliefweb.int/v1/reports/4018475   \n",
       "59954  https://api.reliefweb.int/v1/reports/4018475   \n",
       "59954  https://api.reliefweb.int/v1/reports/4018475   \n",
       "\n",
       "                               sent_idx  \\\n",
       "6      c542bbd546f24a8fadef06c44e00ee75   \n",
       "8      0399d1bd2ee646e998630bbd0f21d05f   \n",
       "10     840d571fff7d4a8aab6c0b914c715c48   \n",
       "23     7006cc561a92471a9ff7ea232bb321cf   \n",
       "25     f4fca5f0152249d09c0e2209288af81f   \n",
       "...                                 ...   \n",
       "59952  3074e8d78ac941f0b5cd75e020a35b58   \n",
       "59952  3074e8d78ac941f0b5cd75e020a35b58   \n",
       "59954  5b9162f2bfbd453fa5e9946b82426b16   \n",
       "59954  5b9162f2bfbd453fa5e9946b82426b16   \n",
       "59954  5b9162f2bfbd453fa5e9946b82426b16   \n",
       "\n",
       "                                         string_sentence         rec_type  \\\n",
       "6      GPC working with AA Epau to distribute 120 dig...  QUANTIFIED_NOUN   \n",
       "8      48 dignity kits being distributed on Mataso is...  QUANTIFIED_NOUN   \n",
       "10     Disability Inclusion in Emergencies Subcluster...  QUANTIFIED_NOUN   \n",
       "23        Inactive EC Greater Port Vila  9, Rural Efate.  QUANTIFIED_NOUN   \n",
       "25     Procurement of the dry rations is underway for...  QUANTIFIED_NOUN   \n",
       "...                                                  ...              ...   \n",
       "59952  The forecasting results will be updated monthl...           ENTITY   \n",
       "59952  The forecasting results will be updated monthl...           ENTITY   \n",
       "59954  The flow volume of the Tonle Sap Lake was slig...           ENTITY   \n",
       "59954  The flow volume of the Tonle Sap Lake was slig...           ENTITY   \n",
       "59954  The flow volume of the Tonle Sap Lake was slig...           ENTITY   \n",
       "\n",
       "                          rec_prefix rec_key       rec_value  \n",
       "6                                        120    dignity kits  \n",
       "8                                         48    dignity kits  \n",
       "10                                        13           March  \n",
       "23     Inactive EC Greater Port Vila       ,     Rural Efate  \n",
       "25                                         2       and areas  \n",
       "...                              ...     ...             ...  \n",
       "59952                                   DATE            2024  \n",
       "59952                                    ORG    Mekong Delta  \n",
       "59954                                    LOC  Tonle Sap Lake  \n",
       "59954                                    ORG             LTA  \n",
       "59954                                   DATE   November 2023  \n",
       "\n",
       "[328689 rows x 7 columns]"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_attributes = pd.concat([df_quants, df_nouns,df_entities])\n",
    "df_attributes = df_attributes.drop(columns=['tmp'])\n",
    "df_attributes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "fb32b4be-93e9-4779-9713-22c3b825dcf2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(343099, 55)"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Join base and attribute df\n",
    "df_joined = df.merge(df_attributes[['sent_idx','rec_type','rec_prefix','rec_key','rec_value']], left_on='sent_idx', right_on='sent_idx', how='left').copy()\n",
    "df_joined.explode('locations')\n",
    "df_joined['locations'] = df_joined['locations'].apply(lambda x: x[0] if len(x)==1 else '')\n",
    "df_joined.explode('dates')\n",
    "df_joined['dates'] = df_joined['dates'].apply(lambda x: x[0] if len(x)==1 else '')\n",
    "\n",
    "\n",
    "df_joined.explode('svot')\n",
    "df_joined.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "b187ad6a-2b12-4fad-b94b-1b231c1d4b82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PRIORITY 1 AREA.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>glide_id</th>\n",
       "      <th>source_level_country</th>\n",
       "      <th>string_sentence</th>\n",
       "      <th>locations</th>\n",
       "      <th>sent_idx</th>\n",
       "      <th>rec_type</th>\n",
       "      <th>rec_prefix</th>\n",
       "      <th>rec_key</th>\n",
       "      <th>rec_value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>39512</th>\n",
       "      <td>TC-2023-000030-VUT</td>\n",
       "      <td>Vanuatu</td>\n",
       "      <td>PRIORITY 1 AREA.</td>\n",
       "      <td></td>\n",
       "      <td>5ee3ddaacc934a0eb08f56822bba51c4</td>\n",
       "      <td>QUANTIFIED_NOUN</td>\n",
       "      <td></td>\n",
       "      <td>1</td>\n",
       "      <td>AREA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39513</th>\n",
       "      <td>TC-2023-000030-VUT</td>\n",
       "      <td>Vanuatu</td>\n",
       "      <td>PRIORITY 1 AREA.</td>\n",
       "      <td></td>\n",
       "      <td>5ee3ddaacc934a0eb08f56822bba51c4</td>\n",
       "      <td>NOUN_SEQUENCE</td>\n",
       "      <td></td>\n",
       "      <td>NOUN_CHUNK</td>\n",
       "      <td>1 AREA</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 glide_id source_level_country   string_sentence locations  \\\n",
       "39512  TC-2023-000030-VUT              Vanuatu  PRIORITY 1 AREA.             \n",
       "39513  TC-2023-000030-VUT              Vanuatu  PRIORITY 1 AREA.             \n",
       "\n",
       "                               sent_idx         rec_type rec_prefix  \\\n",
       "39512  5ee3ddaacc934a0eb08f56822bba51c4  QUANTIFIED_NOUN              \n",
       "39513  5ee3ddaacc934a0eb08f56822bba51c4    NOUN_SEQUENCE              \n",
       "\n",
       "          rec_key rec_value  \n",
       "39512           1      AREA  \n",
       "39513  NOUN_CHUNK    1 AREA  "
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sidx = df_joined['sent_idx'].sample().tolist()[0]\n",
    "print(df_joined['string_sentence'][df_joined['sent_idx'] == sidx].tolist()[0])\n",
    "df_joined[['glide_id','source_level_country','string_sentence','locations','sent_idx','rec_type','rec_prefix','rec_key','rec_value']][df_joined['sent_idx'] == sidx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "89a70efb-e83a-4b58-b452-a504326c51fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_foo = df_joined[['sent_idx','rec_type','rec_value']][df_joined['sent_idx'] == '17c67b978f934355b9e2ebacb1b76ba0']\n",
    "#df_foo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "24621878-c86a-48bf-9bb8-40669c769570",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_duplicates(group):\n",
    "    noun_sequence_values = group[group['rec_type'] == 'NOUN_SEQUENCE']['rec_value']\n",
    "    entity_values = group[group['rec_type'] == 'ENTITY']['rec_value']\n",
    "    mask = ~((group['rec_type'] == 'NOUN_SEQUENCE') & (noun_sequence_values.isin(entity_values)))\n",
    "    return group[mask]\n",
    "\n",
    "# Apply the filtering operation grouped by 'sent_idx'\n",
    "df_joined = df_joined.groupby('sent_idx', group_keys=False).apply(filter_duplicates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "efa9a724-daf4-49ac-949f-8463c2247eaa",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "I/O operation on closed file.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "File \u001b[1;32mC:\\Python311\\Lib\\xml\\etree\\ElementTree.py:801\u001b[0m, in \u001b[0;36m_get_writer\u001b[1;34m(file_or_filename, encoding)\u001b[0m\n\u001b[0;32m    800\u001b[0m stack\u001b[38;5;241m.\u001b[39mcallback(file\u001b[38;5;241m.\u001b[39mdetach)\n\u001b[1;32m--> 801\u001b[0m \u001b[38;5;28;01myield\u001b[39;00m file\u001b[38;5;241m.\u001b[39mwrite, encoding\n",
      "File \u001b[1;32mC:\\Python311\\Lib\\xml\\etree\\ElementTree.py:743\u001b[0m, in \u001b[0;36mElementTree.write\u001b[1;34m(self, file_or_filename, encoding, xml_declaration, default_namespace, method, short_empty_elements)\u001b[0m\n\u001b[0;32m    742\u001b[0m serialize \u001b[38;5;241m=\u001b[39m _serialize[method]\n\u001b[1;32m--> 743\u001b[0m \u001b[43mserialize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mwrite\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_root\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mqnames\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnamespaces\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    744\u001b[0m \u001b[43m          \u001b[49m\u001b[43mshort_empty_elements\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mshort_empty_elements\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mC:\\Python311\\Lib\\xml\\etree\\ElementTree.py:906\u001b[0m, in \u001b[0;36m_serialize_xml\u001b[1;34m(write, elem, qnames, namespaces, short_empty_elements, **kwargs)\u001b[0m\n\u001b[0;32m    905\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m e \u001b[38;5;129;01min\u001b[39;00m elem:\n\u001b[1;32m--> 906\u001b[0m     \u001b[43m_serialize_xml\u001b[49m\u001b[43m(\u001b[49m\u001b[43mwrite\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43me\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mqnames\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    907\u001b[0m \u001b[43m                   \u001b[49m\u001b[43mshort_empty_elements\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mshort_empty_elements\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    908\u001b[0m write(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m</\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m tag \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m>\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mC:\\Python311\\Lib\\xml\\etree\\ElementTree.py:906\u001b[0m, in \u001b[0;36m_serialize_xml\u001b[1;34m(write, elem, qnames, namespaces, short_empty_elements, **kwargs)\u001b[0m\n\u001b[0;32m    905\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m e \u001b[38;5;129;01min\u001b[39;00m elem:\n\u001b[1;32m--> 906\u001b[0m     \u001b[43m_serialize_xml\u001b[49m\u001b[43m(\u001b[49m\u001b[43mwrite\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43me\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mqnames\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    907\u001b[0m \u001b[43m                   \u001b[49m\u001b[43mshort_empty_elements\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mshort_empty_elements\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    908\u001b[0m write(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m</\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m tag \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m>\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "    \u001b[1;31m[... skipping similar frames: _serialize_xml at line 906 (2 times)]\u001b[0m\n",
      "File \u001b[1;32mC:\\Python311\\Lib\\xml\\etree\\ElementTree.py:906\u001b[0m, in \u001b[0;36m_serialize_xml\u001b[1;34m(write, elem, qnames, namespaces, short_empty_elements, **kwargs)\u001b[0m\n\u001b[0;32m    905\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m e \u001b[38;5;129;01min\u001b[39;00m elem:\n\u001b[1;32m--> 906\u001b[0m     \u001b[43m_serialize_xml\u001b[49m\u001b[43m(\u001b[49m\u001b[43mwrite\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43me\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mqnames\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    907\u001b[0m \u001b[43m                   \u001b[49m\u001b[43mshort_empty_elements\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mshort_empty_elements\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    908\u001b[0m write(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m</\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m tag \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m>\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mC:\\Python311\\Lib\\xml\\etree\\ElementTree.py:904\u001b[0m, in \u001b[0;36m_serialize_xml\u001b[1;34m(write, elem, qnames, namespaces, short_empty_elements, **kwargs)\u001b[0m\n\u001b[0;32m    903\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m text:\n\u001b[1;32m--> 904\u001b[0m     write(_escape_cdata(text))\n\u001b[0;32m    905\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m e \u001b[38;5;129;01min\u001b[39;00m elem:\n",
      "\u001b[1;31mMemoryError\u001b[0m: ",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[80], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m output_file \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mD://projects//_external_files//surveyor//files_for_dashboarding//situation_reports_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mgenerate_uuid(\u001b[38;5;241m1\u001b[39m)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.xlsx\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m#output_file = f\"D://projects//_external_files//surveyor//files_for_dashboarding//disaster_reports_{generate_uuid(1)}.xlsx\"\u001b[39;00m\n\u001b[1;32m----> 4\u001b[0m \u001b[43mdf_joined\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_excel\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutput_file\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mD:\\projects\\virtual_environments\\topic_surveyor\\Lib\\site-packages\\pandas\\core\\generic.py:2345\u001b[0m, in \u001b[0;36mNDFrame.to_excel\u001b[1;34m(self, excel_writer, sheet_name, na_rep, float_format, columns, header, index, index_label, startrow, startcol, engine, merge_cells, inf_rep, freeze_panes, storage_options, engine_kwargs)\u001b[0m\n\u001b[0;32m   2332\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mio\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mformats\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mexcel\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ExcelFormatter\n\u001b[0;32m   2334\u001b[0m formatter \u001b[38;5;241m=\u001b[39m ExcelFormatter(\n\u001b[0;32m   2335\u001b[0m     df,\n\u001b[0;32m   2336\u001b[0m     na_rep\u001b[38;5;241m=\u001b[39mna_rep,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   2343\u001b[0m     inf_rep\u001b[38;5;241m=\u001b[39minf_rep,\n\u001b[0;32m   2344\u001b[0m )\n\u001b[1;32m-> 2345\u001b[0m \u001b[43mformatter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwrite\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   2346\u001b[0m \u001b[43m    \u001b[49m\u001b[43mexcel_writer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2347\u001b[0m \u001b[43m    \u001b[49m\u001b[43msheet_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msheet_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2348\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstartrow\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstartrow\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2349\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstartcol\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstartcol\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2350\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfreeze_panes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfreeze_panes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2351\u001b[0m \u001b[43m    \u001b[49m\u001b[43mengine\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2352\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2353\u001b[0m \u001b[43m    \u001b[49m\u001b[43mengine_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mengine_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2354\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mD:\\projects\\virtual_environments\\topic_surveyor\\Lib\\site-packages\\pandas\\io\\formats\\excel.py:965\u001b[0m, in \u001b[0;36mExcelFormatter.write\u001b[1;34m(self, writer, sheet_name, startrow, startcol, freeze_panes, engine, storage_options, engine_kwargs)\u001b[0m\n\u001b[0;32m    962\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    963\u001b[0m     \u001b[38;5;66;03m# make sure to close opened file handles\u001b[39;00m\n\u001b[0;32m    964\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m need_save:\n\u001b[1;32m--> 965\u001b[0m         \u001b[43mwriter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclose\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mD:\\projects\\virtual_environments\\topic_surveyor\\Lib\\site-packages\\pandas\\io\\excel\\_base.py:1374\u001b[0m, in \u001b[0;36mExcelWriter.close\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1372\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mclose\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   1373\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"synonym for save, to make it more file-like\"\"\"\u001b[39;00m\n\u001b[1;32m-> 1374\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_save\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1375\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_handles\u001b[38;5;241m.\u001b[39mclose()\n",
      "File \u001b[1;32mD:\\projects\\virtual_environments\\topic_surveyor\\Lib\\site-packages\\pandas\\io\\excel\\_openpyxl.py:110\u001b[0m, in \u001b[0;36mOpenpyxlWriter._save\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    106\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_save\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    107\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    108\u001b[0m \u001b[38;5;124;03m    Save workbook to disk.\u001b[39;00m\n\u001b[0;32m    109\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 110\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbook\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_handles\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhandle\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    111\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mr+\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_mode \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_handles\u001b[38;5;241m.\u001b[39mhandle, mmap\u001b[38;5;241m.\u001b[39mmmap):\n\u001b[0;32m    112\u001b[0m         \u001b[38;5;66;03m# truncate file to the written content\u001b[39;00m\n\u001b[0;32m    113\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_handles\u001b[38;5;241m.\u001b[39mhandle\u001b[38;5;241m.\u001b[39mtruncate()\n",
      "File \u001b[1;32mD:\\projects\\virtual_environments\\topic_surveyor\\Lib\\site-packages\\openpyxl\\workbook\\workbook.py:386\u001b[0m, in \u001b[0;36mWorkbook.save\u001b[1;34m(self, filename)\u001b[0m\n\u001b[0;32m    384\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwrite_only \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mworksheets:\n\u001b[0;32m    385\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcreate_sheet()\n\u001b[1;32m--> 386\u001b[0m \u001b[43msave_workbook\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mD:\\projects\\virtual_environments\\topic_surveyor\\Lib\\site-packages\\openpyxl\\writer\\excel.py:294\u001b[0m, in \u001b[0;36msave_workbook\u001b[1;34m(workbook, filename)\u001b[0m\n\u001b[0;32m    292\u001b[0m workbook\u001b[38;5;241m.\u001b[39mproperties\u001b[38;5;241m.\u001b[39mmodified \u001b[38;5;241m=\u001b[39m datetime\u001b[38;5;241m.\u001b[39mdatetime\u001b[38;5;241m.\u001b[39mutcnow()\n\u001b[0;32m    293\u001b[0m writer \u001b[38;5;241m=\u001b[39m ExcelWriter(workbook, archive)\n\u001b[1;32m--> 294\u001b[0m \u001b[43mwriter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    295\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[1;32mD:\\projects\\virtual_environments\\topic_surveyor\\Lib\\site-packages\\openpyxl\\writer\\excel.py:275\u001b[0m, in \u001b[0;36mExcelWriter.save\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    273\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msave\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    274\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Write data into the archive.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 275\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwrite_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    276\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_archive\u001b[38;5;241m.\u001b[39mclose()\n",
      "File \u001b[1;32mD:\\projects\\virtual_environments\\topic_surveyor\\Lib\\site-packages\\openpyxl\\writer\\excel.py:77\u001b[0m, in \u001b[0;36mExcelWriter.write_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     74\u001b[0m     custom_override \u001b[38;5;241m=\u001b[39m CustomOverride()\n\u001b[0;32m     75\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmanifest\u001b[38;5;241m.\u001b[39mappend(custom_override)\n\u001b[1;32m---> 77\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_write_worksheets\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     78\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_write_chartsheets()\n\u001b[0;32m     79\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_write_images()\n",
      "File \u001b[1;32mD:\\projects\\virtual_environments\\topic_surveyor\\Lib\\site-packages\\openpyxl\\writer\\excel.py:215\u001b[0m, in \u001b[0;36mExcelWriter._write_worksheets\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    212\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m idx, ws \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mworkbook\u001b[38;5;241m.\u001b[39mworksheets, \u001b[38;5;241m1\u001b[39m):\n\u001b[0;32m    214\u001b[0m     ws\u001b[38;5;241m.\u001b[39m_id \u001b[38;5;241m=\u001b[39m idx\n\u001b[1;32m--> 215\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwrite_worksheet\u001b[49m\u001b[43m(\u001b[49m\u001b[43mws\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    217\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ws\u001b[38;5;241m.\u001b[39m_drawing:\n\u001b[0;32m    218\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_write_drawing(ws\u001b[38;5;241m.\u001b[39m_drawing)\n",
      "File \u001b[1;32mD:\\projects\\virtual_environments\\topic_surveyor\\Lib\\site-packages\\openpyxl\\writer\\excel.py:200\u001b[0m, in \u001b[0;36mExcelWriter.write_worksheet\u001b[1;34m(self, ws)\u001b[0m\n\u001b[0;32m    198\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    199\u001b[0m     writer \u001b[38;5;241m=\u001b[39m WorksheetWriter(ws)\n\u001b[1;32m--> 200\u001b[0m     \u001b[43mwriter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwrite\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    202\u001b[0m ws\u001b[38;5;241m.\u001b[39m_rels \u001b[38;5;241m=\u001b[39m writer\u001b[38;5;241m.\u001b[39m_rels\n\u001b[0;32m    203\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_archive\u001b[38;5;241m.\u001b[39mwrite(writer\u001b[38;5;241m.\u001b[39mout, ws\u001b[38;5;241m.\u001b[39mpath[\u001b[38;5;241m1\u001b[39m:])\n",
      "File \u001b[1;32mD:\\projects\\virtual_environments\\topic_surveyor\\Lib\\site-packages\\openpyxl\\worksheet\\_writer.py:361\u001b[0m, in \u001b[0;36mWorksheetWriter.write\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    359\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwrite_rows()\n\u001b[0;32m    360\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwrite_tail()\n\u001b[1;32m--> 361\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclose\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mD:\\projects\\virtual_environments\\topic_surveyor\\Lib\\site-packages\\openpyxl\\worksheet\\_writer.py:369\u001b[0m, in \u001b[0;36mWorksheetWriter.close\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    365\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    366\u001b[0m \u001b[38;5;124;03mClose the context manager\u001b[39;00m\n\u001b[0;32m    367\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    368\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mxf:\n\u001b[1;32m--> 369\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mxf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclose\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mD:\\projects\\virtual_environments\\topic_surveyor\\Lib\\site-packages\\openpyxl\\worksheet\\_writer.py:289\u001b[0m, in \u001b[0;36mWorksheetWriter.get_stream\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    287\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_stream\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    288\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m xmlfile(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mout) \u001b[38;5;28;01mas\u001b[39;00m xf:\n\u001b[1;32m--> 289\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mwith\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mxf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43melement\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mworksheet\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mxmlns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mSHEET_MAIN_NS\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[0;32m    290\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43;01mtry\u001b[39;49;00m\u001b[43m:\u001b[49m\n\u001b[0;32m    291\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;28;43;01mwhile\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m:\u001b[49m\n",
      "File \u001b[1;32mC:\\Python311\\Lib\\contextlib.py:144\u001b[0m, in \u001b[0;36m_GeneratorContextManager.__exit__\u001b[1;34m(self, typ, value, traceback)\u001b[0m\n\u001b[0;32m    142\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m typ \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    143\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 144\u001b[0m         \u001b[38;5;28mnext\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgen)\n\u001b[0;32m    145\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m:\n\u001b[0;32m    146\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[1;32mD:\\projects\\virtual_environments\\topic_surveyor\\Lib\\site-packages\\et_xmlfile\\xmlfile.py:50\u001b[0m, in \u001b[0;36m_FakeIncrementalFileWriter.element\u001b[1;34m(self, tag, attrib, nsmap, **_extra)\u001b[0m\n\u001b[0;32m     48\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_top_element \u001b[38;5;241m=\u001b[39m parent\n\u001b[0;32m     49\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 50\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_write_element\u001b[49m\u001b[43m(\u001b[49m\u001b[43mel\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     51\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_top_element \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mD:\\projects\\virtual_environments\\topic_surveyor\\Lib\\site-packages\\et_xmlfile\\xmlfile.py:77\u001b[0m, in \u001b[0;36m_FakeIncrementalFileWriter._write_element\u001b[1;34m(self, element)\u001b[0m\n\u001b[0;32m     76\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_write_element\u001b[39m(\u001b[38;5;28mself\u001b[39m, element):\n\u001b[1;32m---> 77\u001b[0m     xml \u001b[38;5;241m=\u001b[39m \u001b[43mtostring\u001b[49m\u001b[43m(\u001b[49m\u001b[43melement\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     78\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_file\u001b[38;5;241m.\u001b[39mwrite(xml)\n",
      "File \u001b[1;32mC:\\Python311\\Lib\\xml\\etree\\ElementTree.py:1098\u001b[0m, in \u001b[0;36mtostring\u001b[1;34m(element, encoding, method, xml_declaration, default_namespace, short_empty_elements)\u001b[0m\n\u001b[0;32m   1084\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Generate string representation of XML element.\u001b[39;00m\n\u001b[0;32m   1085\u001b[0m \n\u001b[0;32m   1086\u001b[0m \u001b[38;5;124;03mAll subelements are included.  If encoding is \"unicode\", a string\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1095\u001b[0m \n\u001b[0;32m   1096\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1097\u001b[0m stream \u001b[38;5;241m=\u001b[39m io\u001b[38;5;241m.\u001b[39mStringIO() \u001b[38;5;28;01mif\u001b[39;00m encoding \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124municode\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m io\u001b[38;5;241m.\u001b[39mBytesIO()\n\u001b[1;32m-> 1098\u001b[0m \u001b[43mElementTree\u001b[49m\u001b[43m(\u001b[49m\u001b[43melement\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwrite\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1099\u001b[0m \u001b[43m                           \u001b[49m\u001b[43mxml_declaration\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mxml_declaration\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1100\u001b[0m \u001b[43m                           \u001b[49m\u001b[43mdefault_namespace\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdefault_namespace\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1101\u001b[0m \u001b[43m                           \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1102\u001b[0m \u001b[43m                           \u001b[49m\u001b[43mshort_empty_elements\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mshort_empty_elements\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1103\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m stream\u001b[38;5;241m.\u001b[39mgetvalue()\n",
      "File \u001b[1;32mC:\\Python311\\Lib\\xml\\etree\\ElementTree.py:731\u001b[0m, in \u001b[0;36mElementTree.write\u001b[1;34m(self, file_or_filename, encoding, xml_declaration, default_namespace, method, short_empty_elements)\u001b[0m\n\u001b[0;32m    729\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    730\u001b[0m         encoding \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mus-ascii\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m--> 731\u001b[0m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mwith\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m_get_writer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile_or_filename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mas\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mwrite\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdeclared_encoding\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[0;32m    732\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mxml\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mand\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mxml_declaration\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\n\u001b[0;32m    733\u001b[0m \u001b[43m            \u001b[49m\u001b[43m(\u001b[49m\u001b[43mxml_declaration\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mand\u001b[39;49;00m\n\u001b[0;32m    734\u001b[0m \u001b[43m             \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlower\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m!=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43municode\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mand\u001b[39;49;00m\n\u001b[0;32m    735\u001b[0m \u001b[43m             \u001b[49m\u001b[43mdeclared_encoding\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlower\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mutf-8\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mus-ascii\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[0;32m    736\u001b[0m \u001b[43m        \u001b[49m\u001b[43mwrite\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m<?xml version=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m1.0\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m encoding=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;132;43;01m%s\u001b[39;49;00m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m?>\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m%\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    737\u001b[0m \u001b[43m            \u001b[49m\u001b[43mdeclared_encoding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mC:\\Python311\\Lib\\contextlib.py:155\u001b[0m, in \u001b[0;36m_GeneratorContextManager.__exit__\u001b[1;34m(self, typ, value, traceback)\u001b[0m\n\u001b[0;32m    153\u001b[0m     value \u001b[38;5;241m=\u001b[39m typ()\n\u001b[0;32m    154\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 155\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgen\u001b[38;5;241m.\u001b[39mthrow(typ, value, traceback)\n\u001b[0;32m    156\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[0;32m    157\u001b[0m     \u001b[38;5;66;03m# Suppress StopIteration *unless* it's the same exception that\u001b[39;00m\n\u001b[0;32m    158\u001b[0m     \u001b[38;5;66;03m# was passed to throw().  This prevents a StopIteration\u001b[39;00m\n\u001b[0;32m    159\u001b[0m     \u001b[38;5;66;03m# raised inside the \"with\" statement from being suppressed.\u001b[39;00m\n\u001b[0;32m    160\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m exc \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m value\n",
      "File \u001b[1;32mC:\\Python311\\Lib\\xml\\etree\\ElementTree.py:773\u001b[0m, in \u001b[0;36m_get_writer\u001b[1;34m(file_or_filename, encoding)\u001b[0m\n\u001b[0;32m    770\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m write, \u001b[38;5;28mgetattr\u001b[39m(file_or_filename, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mencoding\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    771\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    772\u001b[0m     \u001b[38;5;66;03m# wrap a binary writer with TextIOWrapper\u001b[39;00m\n\u001b[1;32m--> 773\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mwith\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mcontextlib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mExitStack\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mas\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mstack\u001b[49m\u001b[43m:\u001b[49m\n\u001b[0;32m    774\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43misinstance\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mfile_or_filename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mio\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mBufferedIOBase\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[0;32m    775\u001b[0m \u001b[43m            \u001b[49m\u001b[43mfile\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mfile_or_filename\u001b[49m\n",
      "File \u001b[1;32mC:\\Python311\\Lib\\contextlib.py:589\u001b[0m, in \u001b[0;36mExitStack.__exit__\u001b[1;34m(self, *exc_details)\u001b[0m\n\u001b[0;32m    585\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    586\u001b[0m     \u001b[38;5;66;03m# bare \"raise exc_details[1]\" replaces our carefully\u001b[39;00m\n\u001b[0;32m    587\u001b[0m     \u001b[38;5;66;03m# set-up context\u001b[39;00m\n\u001b[0;32m    588\u001b[0m     fixed_ctx \u001b[38;5;241m=\u001b[39m exc_details[\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39m__context__\n\u001b[1;32m--> 589\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m exc_details[\u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m    590\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m:\n\u001b[0;32m    591\u001b[0m     exc_details[\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39m__context__ \u001b[38;5;241m=\u001b[39m fixed_ctx\n",
      "File \u001b[1;32mC:\\Python311\\Lib\\contextlib.py:574\u001b[0m, in \u001b[0;36mExitStack.__exit__\u001b[1;34m(self, *exc_details)\u001b[0m\n\u001b[0;32m    572\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m is_sync\n\u001b[0;32m    573\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 574\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mcb\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mexc_details\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[0;32m    575\u001b[0m         suppressed_exc \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m    576\u001b[0m         pending_raise \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[1;32mC:\\Python311\\Lib\\contextlib.py:457\u001b[0m, in \u001b[0;36m_BaseExitStack._create_cb_wrapper.<locals>._exit_wrapper\u001b[1;34m(exc_type, exc, tb)\u001b[0m\n\u001b[0;32m    456\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_exit_wrapper\u001b[39m(exc_type, exc, tb):\n\u001b[1;32m--> 457\u001b[0m     \u001b[43mcallback\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mValueError\u001b[0m: I/O operation on closed file."
     ]
    }
   ],
   "source": [
    "output_file = f\"D://projects//_external_files//surveyor//files_for_dashboarding//situation_reports_{generate_uuid(1)}.xlsx\"\n",
    "#output_file = f\"D://projects//_external_files//surveyor//files_for_dashboarding//disaster_reports_{generate_uuid(1)}.xlsx\"\n",
    "\n",
    "df_joined.iloc[:.to_excel(output_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d898c0f7-a693-49ea-b5b4-5c43da773fa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(time.localtime())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "d19725f3-141b-462f-83ea-b7209644f7d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(298321, 55)"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_joined.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4dc5f6e-932e-4871-96df-c08809609aa9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
